{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "861de7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import psycopg\n",
    "import torch\n",
    "import h5py\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "from psycopg.rows import dict_row\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86882a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"0909231769\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22b5ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index() -> int:\n",
    "    with conn.cursor(row_factory=dict_row) as cur:\n",
    "        cur.execute(\"SELECT COUNT(*) AS total_rows FROM arxiv_papers\")\n",
    "        result = cur.fetchone()\n",
    "    return result['total_rows']\n",
    "\n",
    "def get_row(index: int) -> dict | None:\n",
    "    with conn.cursor(row_factory=dict_row) as cur: \n",
    "        cur.execute(\"SELECT * FROM arxiv_papers WHERE idx = %s\", (index,))\n",
    "        return cur.fetchone()\n",
    "    \n",
    "def get_batch_rows(start_idx: int, end_idx: int) -> list[dict]:\n",
    "    with conn.cursor(row_factory=dict_row) as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT idx, id, abstract \n",
    "            FROM arxiv_papers\n",
    "            WHERE idx >= %s AND idx < %s \n",
    "            ORDER BY idx\n",
    "        \"\"\", (start_idx, end_idx))\n",
    "        return cur.fetchall()\n",
    "    \n",
    "def merge_HDF5_files(input_list, output_file):\n",
    "    if not input_list:\n",
    "        print(\"‚ùå Error: Input file list is empty.\")\n",
    "        return\n",
    "    total_records = 0\n",
    "    first_file = None\n",
    "    for f_path in input_list:\n",
    "        if os.path.exists(f_path):\n",
    "            first_file = f_path\n",
    "            break\n",
    "    if not first_file:\n",
    "        print(\"‚ùå Error: No valid input files found.\")\n",
    "        return\n",
    "    with h5py.File(first_file, 'r') as f_first:\n",
    "        embed_shape = np.squeeze(f_first['embeddings'][:]).shape[1]\n",
    "        embed_dtype = f_first['embeddings'].dtype\n",
    "        url_dtype = f_first['urls'].dtype\n",
    "    with h5py.File(output_file, 'w') as f_output:\n",
    "        f_output.create_dataset(\n",
    "            'urls',\n",
    "            shape=(0,),\n",
    "            maxshape=(None,),\n",
    "            dtype=url_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "        f_output.create_dataset(\n",
    "            'embeddings',\n",
    "            shape=(0, embed_shape),\n",
    "            maxshape=(None, embed_shape),\n",
    "            dtype=embed_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "    pbar = tqdm(total = len(input_list), desc=\"Merging\")\n",
    "    with h5py.File(output_file, 'a') as f_output:\n",
    "        for file_path in input_list:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"‚ö†Ô∏è File not found: {file_path}. Skipping.\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            try:\n",
    "                with h5py.File(file_path, 'r') as f_input:\n",
    "                    current_urls = f_input['urls'][:]\n",
    "                    current_embeddings = f_input['embeddings'][:]\n",
    "                    current_embeddings = np.squeeze(current_embeddings)\n",
    "                    num_records = current_urls.shape[0]\n",
    "                    if num_records == 0:\n",
    "                        continue\n",
    "                    dset_urls = f_output['urls']\n",
    "                    dset_embeddings = f_output['embeddings']\n",
    "                    new_size = total_records + num_records\n",
    "                    dset_urls.resize(new_size, axis=0)\n",
    "                    dset_embeddings.resize(new_size, axis=0)\n",
    "                    dset_urls[total_records:new_size] = current_urls\n",
    "                    dset_embeddings[total_records:new_size] = current_embeddings\n",
    "                    total_records = new_size\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing file {file_path}: {e}. Skipping this file.\")\n",
    "            pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86ec926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = 10000\n",
    "chunk = 1000\n",
    "end = min(end, max_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de85cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST = [  \"openai/clip-vit-base-patch32\",\n",
    "                \"openai/clip-vit-base-patch16\",\n",
    "                \"openai/clip-vit-large-patch14\",\n",
    "                \"openai/clip-vit-large-patch14-336\" ]\n",
    "MODEL = MODEL_LIST[0]\n",
    "CACHE = \"../.cache\"\n",
    "OUTPUT = f\"../.cache/{MODEL[7:]}/paper_embeddings\"\n",
    "NUM_WORKERS = min(16, int(chunk/200))\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fa24d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4ee03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = get_row(2)['abstract']\n",
    "# summarizer = pipeline(\n",
    "#     \"summarization\", \n",
    "#     model=\"t5-base\", \n",
    "#     device=0\n",
    "# )\n",
    "\n",
    "# summary = summarizer(\n",
    "#     text, \n",
    "#     max_new_tokens=60,\n",
    "#     min_length=10, \n",
    "#     do_sample=False\n",
    "# )\n",
    "\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# tokens = tokenizer.encode(summary[0]['summary_text'])\n",
    "# print(f\"S·ªë l∆∞·ª£ng token: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77798f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess.run([\"mkdir\", f\"../.cache/{MODEL[7:]}\"])\n",
    "# subprocess.run([\"mkdir\", OUTPUT])\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-base\", device=0)\n",
    "# processor = CLIPProcessor.from_pretrained(MODEL, use_fast=True)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CLIPModel.from_pretrained(MODEL).to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533fe39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c348ed6fafe041fb9c6d1044e2578520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(original_texts), BATCH_SIZE), desc=\u001b[33m\"\u001b[39m\u001b[33mSummarizing\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     20\u001b[39m     batch_txt = original_texts[batch_idx : batch_idx + BATCH_SIZE]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     summaries = \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_txt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     short_texts.extend([s[\u001b[33m'\u001b[39m\u001b[33msummary_text\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m summaries])\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m summarizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:303\u001b[39m, in \u001b[36mSummarizationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    280\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    282\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m \u001b[33;03m          ids of the summary.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191\u001b[39m, in \u001b[36mText2TextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]], **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m    163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[32m    165\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    193\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    194\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[32m0\u001b[39m])\n\u001b[32m    195\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[32m    196\u001b[39m     ):\n\u001b[32m    197\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/pipelines/base.py:1448\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1445\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1446\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1447\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:126\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:127\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    126\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/pipelines/base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:220\u001b[39m, in \u001b[36mText2TextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    218\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m out_b = output_ids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/generation/utils.py:3260\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3257\u001b[39m beam_indices = running_beam_indices.detach().clone()\n\u001b[32m   3259\u001b[39m \u001b[38;5;66;03m# 4. run the generation loop\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3260\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   3261\u001b[39m     \u001b[38;5;66;03m# a. Forward current tokens, obtain the logits\u001b[39;00m\n\u001b[32m   3262\u001b[39m     flat_running_sequences = \u001b[38;5;28mself\u001b[39m._flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[32m   3263\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/WEB-dev/lib/python3.12/site-packages/transformers/generation/utils.py:2583\u001b[39m, in \u001b[36mGenerationMixin._has_unfinished_sequences\u001b[39m\u001b[34m(self, this_peer_finished, synced_gpus, device)\u001b[39m\n\u001b[32m   2580\u001b[39m         result.past_key_values = result.past_key_values.to_legacy_cache()\n\u001b[32m   2581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m2583\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_has_unfinished_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, this_peer_finished: \u001b[38;5;28mbool\u001b[39m, synced_gpus: \u001b[38;5;28mbool\u001b[39m, device: torch.device) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   2584\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2585\u001b[39m \u001b[33;03m    Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is\u001b[39;00m\n\u001b[32m   2586\u001b[39m \u001b[33;03m    fed through `this_peer_finished`. ZeRO stage 3-friendly.\u001b[39;00m\n\u001b[32m   2587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2588\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m synced_gpus:\n\u001b[32m   2589\u001b[39m         \u001b[38;5;66;03m# Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\u001b[39;00m\n\u001b[32m   2590\u001b[39m         \u001b[38;5;66;03m# The following logic allows an early break if all peers finished generating their sequence\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def clear_gpu():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "for i in range(start, end + 1, chunk):\n",
    "    batch_rows = get_batch_rows(i, i + chunk)\n",
    "    if not batch_rows:\n",
    "        continue\n",
    "    ids = [row['id'] for row in batch_rows]\n",
    "    original_texts = [row[\"abstract\"] for row in batch_rows]\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\", \n",
    "        model=\"t5-base\", \n",
    "        device=0\n",
    "    )\n",
    "    short_texts = []\n",
    "    for batch_idx in tqdm(range(0, len(original_texts), BATCH_SIZE), desc=\"Summarizing\"):\n",
    "        batch_txt = original_texts[batch_idx : batch_idx + BATCH_SIZE]\n",
    "        summaries = summarizer(\n",
    "            batch_txt, \n",
    "            max_new_tokens=60, \n",
    "            min_length=10, \n",
    "            do_sample=False,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            truncation=True\n",
    "        )\n",
    "        short_texts.extend([s['summary_text'] for s in summaries])\n",
    "    del summarizer\n",
    "    clear_gpu()\n",
    "\n",
    "    model = CLIPModel.from_pretrained(MODEL).to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(MODEL, use_fast=True)\n",
    "    model.eval()\n",
    "    all_vectors = []\n",
    "    for batch_idx in tqdm(range(0, len(short_texts), BATCH_SIZE), desc=\"Embedding\"):\n",
    "        batch_txt = short_texts[batch_idx : batch_idx + BATCH_SIZE]\n",
    "        inputs = processor(text=batch_txt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            text_features = model.get_text_features(**inputs)\n",
    "        text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        all_vectors.append(text_features.cpu().numpy())\n",
    "    del model\n",
    "    clear_gpu()\n",
    "\n",
    "    if all_vectors:\n",
    "        final_vectors = np.concatenate(all_vectors, axis=0)\n",
    "        current_urls = [f\"https://arxiv.org/pdf/{id_val}.pdf\" for id_val in ids]\n",
    "        output_path = OUTPUT + f\"/{MODEL[7:]}_Papers_Embedded_{i}_to_{i + chunk - 1}.h5\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        if not os.path.exists(output_path):\n",
    "            with h5py.File(output_path, \"w\") as outfile:\n",
    "                dt = h5py.string_dtype(encoding='utf-8')\n",
    "                outfile.create_dataset(\"urls\", data=current_urls, dtype=dt)\n",
    "                outfile.create_dataset(\"embeddings\", data=final_vectors)\n",
    "            print(f\"   üíæ ƒê√£ l∆∞u file: {os.path.basename(output_path)}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è File ƒë√£ t·ªìn t·∫°i.\")\n",
    "    del short_texts, all_vectors, original_texts, batch_rows\n",
    "    clear_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d68fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WEB-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
