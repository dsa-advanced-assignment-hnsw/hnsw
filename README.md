# HNSW Semantic Search Engine

A powerful semantic search engine that enables searching both images and scientific papers using natural language queries. Built with state-of-the-art ML models (CLIP for images, Sentence Transformers for papers) and HNSW (Hierarchical Navigable Small World) algorithm for fast and accurate similarity search.

## ğŸŒŸ Features

### Image Search
- ğŸ” **Natural Language Search**: Search images using descriptive text queries
- ğŸ–¼ï¸ **Image-to-Image Search**: Upload an image to find visually similar images
- ğŸ¤– **CLIP Embeddings**: State-of-the-art vision-language model by OpenAI
- ğŸŒ **Multi-Source Support**: Search across 1000+ images from Flickr, Pinterest, Google, Meta, Reddit, and more
- ğŸ–¥ï¸ **Dual Versions**: v1 for local images (~100), v2 for online images (~1000, scalable to 1.5M)

### Paper Search (NEW)
- ğŸ“„ **Scientific Paper Search**: Search through 1M+ arXiv papers using semantic queries
- ğŸ“š **Document Upload**: Upload text, PDF, or Markdown files to find similar research papers
- ğŸ”¬ **Sentence Transformers**: High-quality embeddings using all-roberta-large-v1 model
- ğŸ“ **Comprehensive Coverage**: Full arXiv metadata with ~1 million papers indexed

### General
- âš¡ **Fast Similarity Search**: HNSW algorithm for efficient nearest neighbor search
- ğŸ¨ **Modern UI**: Beautiful, responsive interface built with Next.js and Tailwind CSS
- ğŸ“Š **Similarity Scores**: Visual feedback showing match confidence
- ğŸŒ“ **Dark Mode**: Full dark mode support
- ğŸ“± **Flexible Search Modes**: Toggle between different search types seamlessly

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (Next.js)                      â”‚
â”‚  â€¢ React with TypeScript                                   â”‚
â”‚  â€¢ Tailwind CSS for styling                                â”‚
â”‚  â€¢ Dual search interfaces (images & papers)                â”‚
â”‚  â€¢ Deployed on Vercel                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Backend Services (Flask)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Image Search (v1 & v2)   â”‚     Paper Search (NEW)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ CLIP (ViT-B/32)          â”‚ â€¢ Sentence Transformers       â”‚
â”‚ â€¢ Local/Online images      â”‚   (all-roberta-large-v1)      â”‚
â”‚ â€¢ HNSW index (~1000 imgs)  â”‚ â€¢ HNSW index (~1M papers)     â”‚
â”‚ â€¢ Image proxy & caching    â”‚ â€¢ PDF/Text extraction         â”‚
â”‚ â€¢ HDF5 storage             â”‚ â€¢ HDF5 storage (4.1GB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **Backend**: Python 3.8+, conda (recommended)
- **Frontend**: Node.js 18+ or Bun
- **Data**:
  - Image Search: `images_embeds.h5` (v1, ~100 images) or `images_embeds_new.h5` (v2, ~1000 images)
  - Paper Search: `Papers_Embedbed_0-100000.h5` or `Papers_Embedbed_0-1000000.h5` (1M papers, 4.1GB)

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/dsa-advanced-assignment-hnsw.git
cd dsa-advanced-assignment-hnsw
```

### 2. Setup Backend

**Note:** This project uses conda for environment management.

```bash
cd backend

# Activate conda environment
conda activate hnsw-backend-venv

# Install dependencies (if needed)
pip install -r requirements-clean.txt

# Run image search server (v1 - local images)
python server.py

# OR run image search server (v2 - online images, RECOMMENDED)
python server_v2.py

# OR run paper search server (NEW)
python server_paper.py
```

Backend will be available at `http://localhost:5000`

**Choose the appropriate server:**
- `server.py` - For local image search (~100 images)
- `server_v2.py` - For online image search (~1000 images, multi-source)
- `server_paper.py` - For scientific paper search (~100K-1M papers)

### 3. Setup Frontend

```bash
cd client

# Install dependencies
yarn install  # or npm install

# Create environment file
echo "NEXT_PUBLIC_API_URL=http://localhost:5000" > .env.local

# Run development server
yarn dev  # or npm run dev
```

Frontend will be available at `http://localhost:3000`

### 4. Open in Browser

Visit [http://localhost:3000](http://localhost:3000) and start searching!

## ğŸ“š Documentation

- **[Backend README](backend/README.md)** - API documentation and backend setup for all servers
- **[Frontend README](client/README.md)** - UI customization and frontend development
- **[Paper Embedder README](paper_embedder/README.md)** - Generate embeddings for arXiv papers
- **[Image Embedder README](image_embedder/README.md)** - Generate embeddings for Open Images V7 dataset
- **[CLAUDE.md](CLAUDE.md)** - Comprehensive development guide and architecture details

## ğŸ¯ How It Works

### Image Search
1. **Image Preprocessing**: Images are converted to embeddings using CLIP ViT-B/32 model
2. **HNSW Index**: Embeddings are indexed using HNSW for efficient similarity search
3. **Text/Image Query**: User's query (text or image) is converted to embedding using CLIP
4. **Similarity Search**: HNSW finds k-nearest neighbors based on cosine similarity
5. **Results**: Top matching images are returned with similarity scores

### Paper Search
1. **Paper Preprocessing**: Paper abstracts are converted to embeddings using Sentence Transformers
2. **HNSW Index**: 1M+ paper embeddings are indexed for fast retrieval
3. **Text/Document Query**: User's query or uploaded document is converted to embedding
4. **Similarity Search**: HNSW finds k-nearest papers based on semantic similarity
5. **Results**: Top matching papers with arXiv URLs and similarity scores

## ğŸ› ï¸ Technology Stack

### Backend
- **Framework**: Flask 3.0
- **ML Models**:
  - OpenAI CLIP (ViT-B/32) for images - 512-dim embeddings
  - Sentence Transformers (all-roberta-large-v1) for papers - 1024-dim embeddings
- **Vector Search**: hnswlib (HNSW algorithm)
- **Data Storage**: HDF5 (h5py)
- **Deep Learning**: PyTorch
- **Document Processing**: PyPDF2 for PDF text extraction
- **Environment**: Conda for dependency management

### Frontend
- **Framework**: Next.js 15 (with Turbopack)
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **Deployment**: Vercel

### Data Pipeline
- **Image Embedder**: Jupyter Notebook for Open Images V7 dataset processing
- **Paper Embedder**: Jupyter Notebook for arXiv metadata processing

## ğŸ“Š API Endpoints

### Image Search API (server.py / server_v2.py)

#### Search Images by Text
```bash
POST /search
Content-Type: application/json

{
  "query": "beach sunset",
  "k": 20
}
```

#### Search Images by Image
```bash
POST /search/image
Content-Type: multipart/form-data

FormData:
- image: [image file]
- k: 20
```

#### Get Image (v1)
```bash
GET /image/<path>
```

#### Image Proxy (v2)
```bash
GET /image-proxy?url=<encoded_url>
```

#### Cache Management (v2)
```bash
GET /cache/stats      # Get cache statistics
POST /cache/clear     # Clear image cache
```

### Paper Search API (server_paper.py)

#### Search Papers by Text
```bash
POST /search
Content-Type: application/json

{
  "query": "deep learning transformers",
  "k": 20
}
```

#### Search Papers by Document
```bash
POST /search/document
Content-Type: multipart/form-data

FormData:
- document: [.txt, .pdf, or .md file]
- k: 20
```

### Common Endpoints

#### Health Check
```bash
GET /health
```

## ğŸš¢ Deployment

### Frontend (Vercel - Recommended)
```bash
cd client
vercel
# Set environment variable: NEXT_PUBLIC_API_URL=https://your-backend-url.com
```

### Backend Options

**Option 1: Local/VPS with Conda**
```bash
conda activate hnsw-backend-venv
python server_v2.py    # or server_paper.py
# For production: gunicorn -w 1 -b 0.0.0.0:5000 server_v2:app --timeout 120
```

**Option 2: Cloud Platforms (Railway, Render, etc.)**
- Deploy backend directory
- Set environment variables (see CLAUDE.md)
- Use gunicorn for production
- Ensure sufficient memory (3-4GB for large datasets)

**Important:** CORS is pre-configured for cross-origin requests, allowing separate frontend/backend deployment.

## ğŸ“ Project Structure

```
dsa-advanced-assignment-hnsw/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.py                      # Flask API - Image search v1 (local)
â”‚   â”œâ”€â”€ server_v2.py                   # Flask API - Image search v2 (online)
â”‚   â”œâ”€â”€ server_paper.py                # Flask API - Paper search (NEW)
â”‚   â”œâ”€â”€ requirements-clean.txt         # Python dependencies (recommended)
â”‚   â”œâ”€â”€ images_embeds.h5              # Image embeddings v1 (~100 images)
â”‚   â”œâ”€â”€ images_embeds_new.h5          # Image embeddings v2 (~1000 images)
â”‚   â”œâ”€â”€ Papers_Embedbed_0-100000.h5   # Paper embeddings (100K papers)
â”‚   â”œâ”€â”€ Papers_Embedbed_0-1000000.h5  # Paper embeddings (1M papers, 4.1GB)
â”‚   â”œâ”€â”€ search_using_hnsw.ipynb       # Research notebook
â”‚   â”œâ”€â”€ test_image_paths.py           # Test utility for HDF5 files
â”‚   â””â”€â”€ README.md                      # Backend documentation
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ app/
â”‚   â”‚       â””â”€â”€ page.tsx               # Main search interface
â”‚   â”œâ”€â”€ package.json                   # Node dependencies
â”‚   â”œâ”€â”€ vercel.json                   # Vercel configuration
â”‚   â””â”€â”€ README.md                      # Frontend documentation
â”œâ”€â”€ paper_embedder/
â”‚   â”œâ”€â”€ paper_embedder.ipynb          # Generate paper embeddings
â”‚   â””â”€â”€ README.md                      # Paper embedder documentation
â”œâ”€â”€ image_embedder/
â”‚   â”œâ”€â”€ image_embedder.ipynb          # Generate image embeddings
â”‚   â””â”€â”€ README.md                      # Image embedder documentation
â”œâ”€â”€ CLAUDE.md                          # Comprehensive development guide
â””â”€â”€ README.md                          # This file
```

## ğŸ§ª Example Queries

### Image Search
Try these search queries:
- "dog playing in park"
- "beach sunset"
- "mountain landscape"
- "city skyline at night"
- "cat sleeping"

### Paper Search
Try these research queries:
- "deep learning for computer vision"
- "transformer architecture in natural language processing"
- "reinforcement learning algorithms"
- "quantum computing applications"
- "generative adversarial networks"

## ğŸ”§ Configuration

### Backend Configuration (Environment Variables)

**Image Search v2** (`server_v2.py`):
- `PORT` - Server port (default: 5000)
- `FLASK_DEBUG` - Enable debug mode (default: 0)
- `H5_FILE_PATH` - Path to HDF5 file (default: images_embeds_new.h5)
- `MAX_HNSW_ELEMENTS` - HNSW capacity (default: 2000000)
- `IMAGE_CACHE_SIZE_MB` - Cache size in MB (default: 100)
- `IMAGE_FETCH_TIMEOUT` - HTTP timeout in seconds (default: 10)

**Paper Search** (`server_paper.py`):
- `PORT` - Server port (default: 5000)
- `FLASK_DEBUG` - Enable debug mode (default: 0)
- Update HDF5 file path in code to switch between 100K/1M paper datasets

### Frontend Configuration

Edit `client/.env.local`:
```env
NEXT_PUBLIC_API_URL=https://your-backend-url.com
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [OpenAI CLIP](https://github.com/openai/CLIP) for the vision-language model
- [Sentence Transformers](https://www.sbert.net/) for high-quality text embeddings
- [hnswlib](https://github.com/nmslib/hnswlib) for fast approximate nearest neighbor search
- [arXiv](https://arxiv.org/) for providing open access to scientific papers
- [Open Images V7](https://storage.googleapis.com/openimages/web/index.html) for the image dataset
- [Next.js](https://nextjs.org/) for the frontend framework
- [Flask](https://flask.palletsprojects.com/) for the backend framework

## ğŸ“ Support

- ğŸ’¬ Issues: [GitHub Issues](https://github.com/dsa-advanced-assignment-hnsw/hnsw-search-engine/issues)
- ğŸ“– Documentation: See [CLAUDE.md](CLAUDE.md) for detailed development guide

## â­ Star History

If you find this project useful, please consider giving it a star!

---

**Built with â¤ï¸ using CLIP, Sentence Transformers, HNSW, Flask, and Next.js** 