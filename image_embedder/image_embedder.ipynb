{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGE EMBEDDER\n",
    "This notebook implements a complete pipeline to **download a batch of images from a URL list**, use the **OpenAI CLIP (ViT-B/32) model** to **convert them into vector embeddings**, and finally **merge all vectors** into a single HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialization and Setup\n",
    "Installs all necessary Python libraries, including **torch**, **h5py**, **gdown** *(for Google Drive downloads)*, **Pillow** *(for image processing)*, **ipywidgets** *(for tqdm)*, and most importantly, **clip** *(installed directly from OpenAI's GitHub)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-bchimaqz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-bchimaqz\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: boto3 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (1.40.64)\n",
      "Requirement already satisfied: h5py in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (3.15.1)\n",
      "Requirement already satisfied: gdown in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: torch in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: tqdm in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: typing in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: Pillow in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (12.0.0)\n",
      "Requirement already satisfied: ipywidgets in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (8.1.8)\n",
      "Requirement already satisfied: ftfy in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from clip==1.0) (2025.10.23)\n",
      "Requirement already satisfied: torchvision in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from clip==1.0) (0.24.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.64 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from boto3) (1.40.64)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from boto3) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.64->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.64->boto3) (1.17.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from gdown) (4.14.2)\n",
      "Requirement already satisfied: filelock in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from gdown) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/huynguyen/miniconda3/envs/test/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests boto3 h5py gdown torch tqdm typing numpy gdown Pillow ipywidgets git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "import clip\n",
    "import gdown\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data and Tool Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads the `downloader.py` utility script, part of the **Open Images dataset** toolkit. This script is used for efficient, parallel downloading of the dataset images and is stored in the `.cache` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T11:42:38.409642Z",
     "iopub.status.busy": "2025-11-02T11:42:38.409065Z",
     "iopub.status.idle": "2025-11-02T11:44:05.777123Z",
     "shell.execute_reply": "2025-11-02T11:44:05.776371Z",
     "shell.execute_reply.started": "2025-11-02T11:42:38.409599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\".cache\", exist_ok=True)\n",
    "if not os.path.isfile(\".cache/downloader.py\"):\n",
    "    command = [\"wget\", \"-O\", \".cache/downloader.py\", \"https://raw.githubusercontent.com/openimages/dataset/master/downloader.py\"]\n",
    "    subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads metadata files (`image_ids.json`, `image_urls.json`) derived from the **Open Images V7 dataset**, hosted on a Google Drive mirror. This data is used in accordance with the original **CC BY 4.0 License**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"RAW_DATASET\", exist_ok=True)\n",
    "if not os.path.isfile(\"RAW_DATASET/image_ids.json\"):\n",
    "    gdown.download(id='1-HcMviWpMn84cDaDkDpPXqEr-6BFc0bv', output='RAW_DATASET/image_ids.json', quiet=False)\n",
    "if not os.path.isfile(\"RAW_DATASET/image_urls.json\"):\n",
    "    gdown.download(id='1aaeCYKWQFva8M-ene1whUyZReSnzjoLA', output='RAW_DATASET/image_urls.json', quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Loading and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T11:45:04.134917Z",
     "iopub.status.busy": "2025-11-02T11:45:04.134291Z",
     "iopub.status.idle": "2025-11-02T11:45:04.138852Z",
     "shell.execute_reply": "2025-11-02T11:45:04.138086Z",
     "shell.execute_reply.started": "2025-11-02T11:45:04.134890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_json(ten_file):\n",
    "    with open(ten_file, 'r', encoding='utf-8') as f:\n",
    "        du_lieu = json.load(f)\n",
    "        return du_lieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_IDs = read_json(\"RAW_DATASET/image_ids.json\")\n",
    "image_URLs = read_json(\"RAW_DATASET/image_urls.json\")\n",
    "n_images = len(image_URLs)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(i):\n",
    "    \"\"\"\n",
    "    Hàm này đọc file, preprocess và trả về tensor trên CPU.\n",
    "    \"\"\"\n",
    "    image_path = f\".cache/Images/{image_IDs[i]}.jpg\"\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            # 'preprocess' là hàm của CLIP, trả về tensor trên CPU\n",
    "            image_tensor = preprocess(image) \n",
    "            return image_URLs[i], image_tensor\n",
    "        except Exception as e:\n",
    "            # Bỏ qua các ảnh bị hỏng\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_HDF5_files(input_list, output_file):\n",
    "    \"\"\"\n",
    "    Gộp dữ liệu từ nhiều file HDF5 (có cùng cấu trúc: urls, embeddings)\n",
    "    \"\"\"\n",
    "    if not input_list:\n",
    "        print(\"❌ Error: Input file list is empty.\")\n",
    "        return\n",
    "\n",
    "    total_records = 0\n",
    "\n",
    "    # 1. Xử lý file đầu tiên để xác định cấu trúc và khởi tạo Dataset\n",
    "    first_file = None\n",
    "    for f_path in input_list:\n",
    "        if os.path.exists(f_path):\n",
    "            first_file = f_path\n",
    "            break\n",
    "\n",
    "    if not first_file:\n",
    "        print(\"❌ Error: No valid input files found.\")\n",
    "        return\n",
    "\n",
    "    with h5py.File(first_file, 'r') as f_first:\n",
    "        # Lấy thông tin dtype và chiều (dim)\n",
    "        embed_shape = np.squeeze(f_first['embeddings'][:]).shape[1]\n",
    "        embed_dtype = f_first['embeddings'].dtype\n",
    "        url_dtype = f_first['urls'].dtype\n",
    "\n",
    "    with h5py.File(output_file, 'w') as f_output:\n",
    "        f_output.create_dataset(\n",
    "            'urls',\n",
    "            shape=(0,),\n",
    "            maxshape=(None,),\n",
    "            dtype=url_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "        f_output.create_dataset(\n",
    "            'embeddings',\n",
    "            shape=(0, embed_shape),\n",
    "            maxshape=(None, embed_shape),\n",
    "            dtype=embed_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "\n",
    "    pbar = tqdm(total = len(input_list), desc=\"Merging\")\n",
    "\n",
    "    # 2. Lặp và Gộp dữ liệu vào file Output (sử dụng chế độ 'a')\n",
    "    with h5py.File(output_file, 'a') as f_output:\n",
    "        for file_path in input_list:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"⚠️ File not found: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with h5py.File(file_path, 'r') as f_input:\n",
    "                    current_urls = f_input['urls'][:]\n",
    "                    current_embeddings = f_input['embeddings'][:]\n",
    "                    current_embeddings = np.squeeze(current_embeddings)\n",
    "                    num_records = current_urls.shape[0]\n",
    "\n",
    "                    if num_records == 0:\n",
    "                        continue\n",
    "\n",
    "                    dset_urls = f_output['urls']\n",
    "                    dset_embeddings = f_output['embeddings']\n",
    "\n",
    "                    new_size = total_records + num_records\n",
    "\n",
    "                    # Thay đổi kích thước (Resize) Dataset trong file output\n",
    "                    dset_urls.resize(new_size, axis=0)\n",
    "                    dset_embeddings.resize(new_size, axis=0)\n",
    "\n",
    "                    # Ghi dữ liệu vào khoảng trống vừa resize\n",
    "                    dset_urls[total_records:new_size] = current_urls\n",
    "                    dset_embeddings[total_records:new_size] = current_embeddings\n",
    "\n",
    "                    # Cập nhật tổng số bản ghi\n",
    "                    total_records = new_size\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing file {file_path}: {e}. Skipping this file.\")\n",
    "\n",
    "            pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Main Processing Pipeline (Loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the main logic is executed. The pipeline runs in **chunks** to save memory and be fault-tolerant.\n",
    "* For **each chunk**, the script will:\n",
    "    1.  **Download Images:** Call the `downloader.py` script to download the images for the current chunk.\n",
    "    2.  **Preprocess (CPU):** Use a `ThreadPoolExecutor` to run the `load_and_preprocess_image` function in parallel.\n",
    "    3.  **Encode (GPU):** Gather preprocessed images into large batches, push them to the GPU, and run `model.encode_image` to get the vectors.\n",
    "    4.  **Save Chunk:** Write the URLs and vectors for this chunk into a temporary HDF5 file (e.g., `OUTPUT/Images_Embedded_0.h5`).\n",
    "    5.  **Cleanup Chunk:** Delete the temporary image directory (`.cache/Images`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0\n",
    "END = n_images\n",
    "CHUNK = 10000\n",
    "GPU_BATCH_SIZE = 512 \n",
    "NUM_WORKERS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T11:46:39.544925Z",
     "iopub.status.busy": "2025-11-02T11:46:39.544412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"OUTPUT\", exist_ok=True) \n",
    "\n",
    "pbar = tqdm(total = END - START, desc = \"Embedding: \")\n",
    "\n",
    "for x in range(START, END, CHUNK):\n",
    "\n",
    "    with open(\".cache/list_images.txt\", 'w', encoding='utf-8') as file:\n",
    "        for i in range(x, min(n_images, x + CHUNK)):\n",
    "            file.write(\"train/\" + image_IDs[i] + \"\\n\")\n",
    "\n",
    "    # Tải ảnh\n",
    "    os.makedirs(\".cache/Images\", exist_ok=True)\n",
    "    subprocess.run([\n",
    "        \"python\", \".cache/downloader.py\", \".cache/list_images.txt\",\n",
    "        \"--download_folder=.cache/Images\", \"--num_processes=100\"\n",
    "    ])\n",
    "\n",
    "    # 1. Dùng ThreadPool để ĐỌC và PREPROCESS (CPU)\n",
    "    preprocessed_data_cpu = [] \n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        futures = [executor.submit(load_and_preprocess_image, i) for i in range(x, min(n_images, x + CHUNK))]\n",
    "        \n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                preprocessed_data_cpu.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Nếu không có ảnh nào được xử lý trong chunk này, bỏ qua\n",
    "    if not preprocessed_data_cpu:\n",
    "        print(f\"No images were processed in chunk {int(x / CHUNK)}\")\n",
    "        subprocess.run([\"rm\", \"-rf\", \".cache/Images\"])\n",
    "        continue\n",
    "\n",
    "    # 2. Xử lý theo LÔ (BATCH) trên GPU\n",
    "    final_urls = []\n",
    "    final_embeddings_list = []\n",
    "\n",
    "    urls_cpu = [item[0] for item in preprocessed_data_cpu]\n",
    "    tensors_cpu = [item[1] for item in preprocessed_data_cpu]\n",
    "\n",
    "    # Tắt tính toán gradient (RẤT QUAN TRỌNG khi inference)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(tensors_cpu), GPU_BATCH_SIZE):\n",
    "            batch_tensors_cpu = tensors_cpu[i : i + GPU_BATCH_SIZE]\n",
    "            batch_urls = urls_cpu[i : i + GPU_BATCH_SIZE]\n",
    "            \n",
    "            batch_on_cpu = torch.stack(batch_tensors_cpu)\n",
    "            batch_on_gpu = batch_on_cpu.to(device)\n",
    "\n",
    "            vectors_gpu = model.encode_image(batch_on_gpu)\n",
    "            vectors_gpu = vectors_gpu / vectors_gpu.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            vectors_numpy = vectors_gpu.cpu().detach().numpy().astype(np.float32)\n",
    "\n",
    "            final_urls.extend(batch_urls)\n",
    "            final_embeddings_list.append(vectors_numpy)\n",
    "\n",
    "    # 3. GHI FILE HDF5\n",
    "    output_path = f\"OUTPUT/Images_Embedded_{int(x / CHUNK)}.h5\"\n",
    "    if not os.path.exists(output_path) and final_embeddings_list:\n",
    "        all_embeddings_numpy = np.vstack(final_embeddings_list)\n",
    "        \n",
    "        with h5py.File(output_path, \"w\") as outfile:\n",
    "            outfile.create_dataset(\"urls\", data=np.array(final_urls, dtype='S'))\n",
    "            outfile.create_dataset(\"embeddings\", data=all_embeddings_numpy)\n",
    "\n",
    "    # 4. XÓA TEMP IMAGES\n",
    "    subprocess.run([\"rm\", \"-rf\", \".cache/Images\"])\n",
    "\n",
    "pbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge:** Call `merge_HDF5_files` to combine all temporary HDF5 chunk files into a single final file: `OUTPUT/Images_Embedded.h5`.\n",
    "**Final Cleanup:** Delete all the temporary HDF5 chunk files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gộp các file HDF5\n",
    "file_chunks = [\n",
    "    f\"OUTPUT/Images_Embedded_{int(x / CHUNK)}.h5\"\n",
    "    for x in range(START, END, CHUNK)\n",
    "]\n",
    "file_gop_cuoi = f\"OUTPUT/Images_Embedded.h5\"\n",
    "merge_HDF5_files(file_chunks, file_gop_cuoi)\n",
    "\n",
    "# Xóa các file chunk\n",
    "for x in range(START, END, CHUNK):\n",
    "    os.remove(f\"OUTPUT/Images_Embedded_{int(x / CHUNK)}.h5\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8629323,
     "sourceId": 13582735,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
