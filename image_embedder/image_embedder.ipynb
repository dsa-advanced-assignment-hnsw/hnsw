{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGE EMBEDDER\n",
    "This notebook implements a complete pipeline to **download a batch of images from a URL list**, use the **OpenAI CLIP (ViT-B/32) model** to **convert them into vector embeddings**, and finally **merge all vectors** into a single HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialization and Setup\n",
    "Installs all necessary Python libraries, including **torch**, **h5py**, **gdown** *(for Google Drive downloads)*, **Pillow** *(for image processing)*, **ipywidgets** *(for tqdm)*, and most importantly, **clip** *(installed directly from OpenAI's GitHub)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests boto3 h5py gdown torch tqdm typing numpy gdown Pillow ipywidgets git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "import clip\n",
    "import gdown\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data and Tool Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads the `downloader.py` utility script, part of the **Open Images dataset** toolkit. This script is used for efficient, parallel downloading of the dataset images and is stored in the `.cache` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T11:42:38.409642Z",
     "iopub.status.busy": "2025-11-02T11:42:38.409065Z",
     "iopub.status.idle": "2025-11-02T11:44:05.777123Z",
     "shell.execute_reply": "2025-11-02T11:44:05.776371Z",
     "shell.execute_reply.started": "2025-11-02T11:42:38.409599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure the local cache directory exists.\n",
    "os.makedirs(\".cache\", exist_ok=True)\n",
    "\n",
    "# Check if the downloader script already exists in the cache.\n",
    "if not os.path.isfile(\".cache/downloader.py\"):\n",
    "    # If not present, download the script from the remote repository.\n",
    "    command = [\"wget\", \"-O\", \".cache/downloader.py\", \"https://raw.githubusercontent.com/openimages/dataset/master/downloader.py\"]\n",
    "    # Execute the download command and ensure it runs successfully.\n",
    "    subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads metadata files (`image_ids.json`, `image_urls.json`) derived from the **Open Images V7 dataset**, hosted on a Google Drive mirror. This data is used in accordance with the original **CC BY 4.0 License**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main directory for raw dataset storage.\n",
    "# 'exist_ok=True' prevents errors if the folder is already present.\n",
    "os.makedirs(\"RAW_DATASET\", exist_ok=True)\n",
    "\n",
    "# Check if the 'image_ids' file exists. If not, download it from Google Drive.\n",
    "if not os.path.isfile(\"RAW_DATASET/image_ids.json\"):\n",
    "    # Download file using its Google Drive ID.\n",
    "    gdown.download(id='1-HcMviWpMn84cDaDkDpPXqEr-6BFc0bv', output='RAW_DATASET/image_ids.json', quiet=False)\n",
    "\n",
    "# Check if the 'image_urls' file exists. If not, download it from Google Drive.\n",
    "if not os.path.isfile(\"RAW_DATASET/image_urls.json\"):\n",
    "    # Download file using its Google Drive ID.\n",
    "    gdown.download(id='1aaeCYKWQFva8M-ene1whUyZReSnjjoLA', output='RAW_DATASET/image_urls.json', quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Loading and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T11:45:04.134917Z",
     "iopub.status.busy": "2025-11-02T11:45:04.134291Z",
     "iopub.status.idle": "2025-11-02T11:45:04.138852Z",
     "shell.execute_reply": "2025-11-02T11:45:04.138086Z",
     "shell.execute_reply.started": "2025-11-02T11:45:04.134890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    # Open the JSON file for reading, ensuring UTF-8 support.\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Load and return the data as a Python list.\n",
    "        data = json.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image identifiers from the local JSON file.\n",
    "image_IDs = read_json(\"RAW_DATASET/image_ids.json\")\n",
    "\n",
    "# Load the corresponding image URLs from the local JSON file.\n",
    "image_URLs = read_json(\"RAW_DATASET/image_urls.json\")\n",
    "\n",
    "# Determine the total number of images to be processed (based on the number of URLs).\n",
    "n_images = len(image_URLs)\n",
    "\n",
    "# Set the device for computation: use GPU (\"cuda\") if available, otherwise fallback to CPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the pre-trained CLIP model (ViT-B/32) onto the selected device.\n",
    "# 'model' is the main neural network, and 'preprocess' is the required image transformation pipeline.\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(i):\n",
    "    \"\"\"\n",
    "    Loads an image file, applies necessary preprocessing (from CLIP), \n",
    "    and returns the resulting tensor along with its URL.\n",
    "    \"\"\"\n",
    "    # Construct the full path to the image using its ID.\n",
    "    image_path = f\".cache/Images/{image_IDs[i]}.jpg\"\n",
    "    \n",
    "    # Check if the file exists on the local filesystem.\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            # Open the image file using PIL.\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # Apply the standard CLIP preprocessing pipeline.\n",
    "            # 'preprocess' returns a PyTorch tensor (typically on CPU).\n",
    "            image_tensor = preprocess(image) \n",
    "            \n",
    "            # Return the original URL and the processed tensor.\n",
    "            return image_URLs[i], image_tensor\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Catch exceptions (e.g., File is corrupted, not a valid image format).\n",
    "            # Print an error message and skip the problematic image.\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Return None if the image file was not found at the specified path.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_HDF5_files(input_list, output_file):\n",
    "    \"\"\"\n",
    "    Merges data (URLs and embeddings) from multiple HDF5 files into a single output file.\n",
    "    \"\"\"\n",
    "    if not input_list:\n",
    "        print(\"❌ Error: Input file list is empty.\")\n",
    "        return\n",
    "\n",
    "    total_records = 0\n",
    "\n",
    "    # 1. Initialize Output Structure based on the first valid file\n",
    "    first_file = None\n",
    "    # Find the first existing file to determine the required structure (dtype, shape).\n",
    "    for f_path in input_list:\n",
    "        if os.path.exists(f_path):\n",
    "            first_file = f_path\n",
    "            break\n",
    "\n",
    "    if not first_file:\n",
    "        print(\"❌ Error: No valid input files found.\")\n",
    "        return\n",
    "\n",
    "    with h5py.File(first_file, 'r') as f_first:\n",
    "        # Get embedding dimension (shape[1]) and data types (dtype) for initialization.\n",
    "        # np.squeeze is used to handle potential extra dimensions (e.g., shape (N, 1, D) -> (N, D)).\n",
    "        embed_shape = np.squeeze(f_first['embeddings'][:]).shape[1]\n",
    "        embed_dtype = f_first['embeddings'].dtype\n",
    "        url_dtype = f_first['urls'].dtype\n",
    "\n",
    "    # Create the output file and initialize extendable datasets.\n",
    "    with h5py.File(output_file, 'w') as f_output:\n",
    "        # Initialize 'urls' dataset with zero length, maxshape=(None,) allows extension.\n",
    "        f_output.create_dataset(\n",
    "            'urls',\n",
    "            shape=(0,),\n",
    "            maxshape=(None,),\n",
    "            dtype=url_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "        # Initialize 'embeddings' dataset with zero length, maxshape=(None, embed_shape) allows extension.\n",
    "        f_output.create_dataset(\n",
    "            'embeddings',\n",
    "            shape=(0, embed_shape),\n",
    "            maxshape=(None, embed_shape),\n",
    "            dtype=embed_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "\n",
    "    pbar = tqdm(total = len(input_list), desc=\"Merging\")\n",
    "\n",
    "    # 2. Loop through input files and append data\n",
    "    # Open the output file in append mode ('a') for modification.\n",
    "    with h5py.File(output_file, 'a') as f_output:\n",
    "        for file_path in input_list:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"⚠️ File not found: {file_path}. Skipping.\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with h5py.File(file_path, 'r') as f_input:\n",
    "                    # Read data from the current input file.\n",
    "                    current_urls = f_input['urls'][:]\n",
    "                    current_embeddings = f_input['embeddings'][:]\n",
    "                    current_embeddings = np.squeeze(current_embeddings)\n",
    "                    num_records = current_urls.shape[0]\n",
    "\n",
    "                    if num_records == 0:\n",
    "                        continue\n",
    "\n",
    "                    dset_urls = f_output['urls']\n",
    "                    dset_embeddings = f_output['embeddings']\n",
    "\n",
    "                    new_size = total_records + num_records\n",
    "\n",
    "                    # Resize the datasets in the output file to accommodate new records.\n",
    "                    dset_urls.resize(new_size, axis=0)\n",
    "                    dset_embeddings.resize(new_size, axis=0)\n",
    "\n",
    "                    # Write the current file's data into the newly reserved space.\n",
    "                    dset_urls[total_records:new_size] = current_urls\n",
    "                    dset_embeddings[total_records:new_size] = current_embeddings\n",
    "\n",
    "                    # Update the running total of merged records.\n",
    "                    total_records = new_size\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle potential errors during file reading or resizing.\n",
    "                print(f\"❌ Error processing file {file_path}: {e}. Skipping this file.\")\n",
    "\n",
    "            pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Main Processing Pipeline (Loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the main logic is executed. The pipeline runs in **chunks** to save memory and be fault-tolerant.\n",
    "* For **each chunk**, the script will:\n",
    "    1.  **Download Images:** Call the `downloader.py` script to download the images for the current chunk.\n",
    "    2.  **Preprocess (CPU):** Use a `ThreadPoolExecutor` to run the `load_and_preprocess_image` function in parallel.\n",
    "    3.  **Encode (GPU):** Gather preprocessed images into large batches, push them to the GPU, and run `model.encode_image` to get the vectors.\n",
    "    4.  **Save Chunk:** Write the URLs and vectors for this chunk into a temporary HDF5 file (e.g., `OUTPUT/Images_Embedded_0.h5`).\n",
    "    5.  **Cleanup Chunk:** Delete the temporary image directory (`.cache/Images`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0\n",
    "END = n_images\n",
    "CHUNK = 10000\n",
    "GPU_BATCH_SIZE = 512 \n",
    "NUM_WORKERS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T11:46:39.544925Z",
     "iopub.status.busy": "2025-11-02T11:46:39.544412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure the main output directory for embedded HDF5 files exists.\n",
    "os.makedirs(\"OUTPUT\", exist_ok=True) \n",
    "\n",
    "# Initialize a progress bar for monitoring the total images to be embedded.\n",
    "pbar = tqdm(total = END - START, desc = \"Embedding: \")\n",
    "\n",
    "# Main loop: Iterate over the image indices in defined CHUNK sizes.\n",
    "for x in range(START, END, CHUNK):\n",
    "\n",
    "    # 1. DOWNLOAD IMAGES FOR CURRENT CHUNK    \n",
    "    # Generate a temporary list file containing image IDs for the current chunk.\n",
    "    with open(\".cache/list_images.txt\", 'w', encoding='utf-8') as file:\n",
    "        for i in range(x, min(n_images, x + CHUNK)):\n",
    "            # Format: 'train/<image_id>'\n",
    "            file.write(\"train/\" + image_IDs[i] + \"\\n\")\n",
    "\n",
    "    # Ensure the target image download directory exists.\n",
    "    os.makedirs(\".cache/Images\", exist_ok=True)\n",
    "    \n",
    "    # Execute the external Python downloader script.\n",
    "    # It reads the list_images.txt and downloads files into .cache/Images using 100 processes.\n",
    "    subprocess.run([\n",
    "        \"python\", \".cache/downloader.py\", \".cache/list_images.txt\",\n",
    "        \"--download_folder=.cache/Images\", \"--num_processes=100\"\n",
    "    ])\n",
    "\n",
    "    # 2. READ & PREPROCESS IMAGES (CPU - Multi-threading)\n",
    "    # List to store (URL, preprocessed_tensor_on_CPU) tuples.\n",
    "    preprocessed_data_cpu = [] \n",
    "    \n",
    "    # Use ThreadPoolExecutor to parallelize I/O and image loading/preprocessing.\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        # Submit the load_and_preprocess_image function for each image index in the chunk.\n",
    "        futures = [executor.submit(load_and_preprocess_image, i) for i in range(x, min(n_images, x + CHUNK))]\n",
    "        \n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                # Append only successful results (not None).\n",
    "                preprocessed_data_cpu.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Skip the current chunk if no images were successfully processed (e.g., all failed/corrupted).\n",
    "    if not preprocessed_data_cpu:\n",
    "        print(f\"No images were processed in chunk {int(x / CHUNK)}\")\n",
    "        # Clean up the downloaded images before continuing.\n",
    "        subprocess.run([\"rm\", \"-rf\", \".cache/Images\"])\n",
    "        continue\n",
    "\n",
    "    # 3. EMBEDDING CALCULATION (GPU - Batch Processing)\n",
    "    final_urls = []\n",
    "    final_embeddings_list = []\n",
    "\n",
    "    # Separate URLs and Tensors for batch processing.\n",
    "    urls_cpu = [item[0] for item in preprocessed_data_cpu]\n",
    "    tensors_cpu = [item[1] for item in preprocessed_data_cpu]\n",
    "\n",
    "    # Disable gradient computation - CRITICAL for memory efficiency during inference.\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the preprocessed tensors in GPU_BATCH_SIZE chunks.\n",
    "        for i in range(0, len(tensors_cpu), GPU_BATCH_SIZE):\n",
    "            batch_tensors_cpu = tensors_cpu[i : i + GPU_BATCH_SIZE]\n",
    "            batch_urls = urls_cpu[i : i + GPU_BATCH_SIZE]\n",
    "            \n",
    "            # Stack individual tensors into a single batch tensor on the CPU.\n",
    "            batch_on_cpu = torch.stack(batch_tensors_cpu)\n",
    "            # Move the entire batch to the designated device (GPU).\n",
    "            batch_on_gpu = batch_on_cpu.to(device)\n",
    "\n",
    "            # Generate image embeddings using the CLIP model.\n",
    "            vectors_gpu = model.encode_image(batch_on_gpu)\n",
    "            \n",
    "            # Normalize the vectors (L2 normalization), required for standard CLIP usage.\n",
    "            vectors_gpu = vectors_gpu / vectors_gpu.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Convert GPU tensor to numpy array (CPU) for HDF5 writing.\n",
    "            vectors_numpy = vectors_gpu.cpu().detach().numpy().astype(np.float32)\n",
    "\n",
    "            # Store results for final HDF5 file.\n",
    "            final_urls.extend(batch_urls)\n",
    "            final_embeddings_list.append(vectors_numpy)\n",
    "\n",
    "    # 4. WRITE HDF5 FILE\n",
    "    # Construct the output path for the current chunk's embedding file.\n",
    "    output_path = f\"OUTPUT/Images_Embedded_{int(x / CHUNK)}.h5\"\n",
    "    \n",
    "    # Only write if the file doesn't exist and we have data to write.\n",
    "    if not os.path.exists(output_path) and final_embeddings_list:\n",
    "        # Combine all batch numpy arrays into a single large array.\n",
    "        all_embeddings_numpy = np.vstack(final_embeddings_list)\n",
    "        \n",
    "        # Write the URLs and embeddings into a new HDF5 file.\n",
    "        with h5py.File(output_path, \"w\") as outfile:\n",
    "            # URLs are stored as byte strings (dtype='S') in HDF5.\n",
    "            outfile.create_dataset(\"urls\", data=np.array(final_urls, dtype='S'))\n",
    "            outfile.create_dataset(\"embeddings\", data=all_embeddings_numpy)\n",
    "\n",
    "    # 5. CLEANUP\n",
    "    # Delete the downloaded images for the current chunk to save disk space.\n",
    "    subprocess.run([\"rm\", \"-rf\", \".cache/Images\"])\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge:** Call `merge_HDF5_files` to combine all temporary HDF5 chunk files into a single final file: `OUTPUT/Images_Embedded.h5`.\n",
    "**Final Cleanup:** Delete all the temporary HDF5 chunk files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MERGE CHUNKED HDF5 FILES\n",
    "# Generate a list of all HDF5 chunk files created during the embedding process.\n",
    "file_chunks = [\n",
    "    f\"OUTPUT/Images_Embedded_{int(x / CHUNK)}.h5\"\n",
    "    for x in range(START, END, CHUNK)\n",
    "]\n",
    "\n",
    "# Define the final, consolidated HDF5 file path.\n",
    "file_gop_cuoi = f\"OUTPUT/Images_Embedded.h5\"\n",
    "\n",
    "# Call the function to merge all chunk files into the single final file.\n",
    "merge_HDF5_files(file_chunks, file_gop_cuoi)\n",
    "\n",
    "# 2. CLEANUP: REMOVE TEMPORARY CHUNKS\n",
    "# Iterate through the indices used to generate the chunk files.\n",
    "for x in range(START, END, CHUNK):\n",
    "    chunk_path = f\"OUTPUT/Images_Embedded_{int(x / CHUNK)}.h5\"\n",
    "    try:\n",
    "        # Delete the individual temporary HDF5 chunk file to free up disk space.\n",
    "        os.remove(chunk_path)\n",
    "    except FileNotFoundError:\n",
    "        # Handle case where the file might have been skipped or already deleted.\n",
    "        print(f\"⚠️ Warning: Chunk file not found during cleanup: {chunk_path}\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8629323,
     "sourceId": 13582735,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
