{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ea2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import psycopg\n",
    "import torch\n",
    "import h5py\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from psycopg.rows import dict_row\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f935e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"0909231769\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071e29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index() -> int:\n",
    "    with conn.cursor(row_factory=dict_row) as cur:\n",
    "        cur.execute(\"SELECT COUNT(*) AS total_rows FROM raw_image\")\n",
    "        result = cur.fetchone()\n",
    "    return result['total_rows']\n",
    "\n",
    "def get_row(index: int) -> dict | None:\n",
    "    with conn.cursor(row_factory=dict_row) as cur: \n",
    "        cur.execute(\"SELECT * FROM raw_image WHERE idx = %s\", (index,))\n",
    "        return cur.fetchone()\n",
    "    \n",
    "def get_batch_rows(start_idx: int, end_idx: int) -> list[dict]:\n",
    "    with conn.cursor(row_factory=dict_row) as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT idx, imageid, subset, originalurl \n",
    "            FROM raw_image \n",
    "            WHERE idx >= %s AND idx < %s \n",
    "            ORDER BY idx\n",
    "        \"\"\", (start_idx, end_idx))\n",
    "        return cur.fetchall()\n",
    "    \n",
    "def merge_HDF5_files(input_list, output_file):\n",
    "    \"\"\"\n",
    "    Merges data (URLs and embeddings) from multiple HDF5 files into a single output file.\n",
    "    \"\"\"\n",
    "    if not input_list:\n",
    "        print(\"❌ Error: Input file list is empty.\")\n",
    "        return\n",
    "\n",
    "    total_records = 0\n",
    "\n",
    "    # 1. Initialize Output Structure based on the first valid file\n",
    "    first_file = None\n",
    "    # Find the first existing file to determine the required structure (dtype, shape).\n",
    "    for f_path in input_list:\n",
    "        if os.path.exists(f_path):\n",
    "            first_file = f_path\n",
    "            break\n",
    "\n",
    "    if not first_file:\n",
    "        print(\"❌ Error: No valid input files found.\")\n",
    "        return\n",
    "\n",
    "    with h5py.File(first_file, 'r') as f_first:\n",
    "        # Get embedding dimension (shape[1]) and data types (dtype) for initialization.\n",
    "        # np.squeeze is used to handle potential extra dimensions (e.g., shape (N, 1, D) -> (N, D)).\n",
    "        embed_shape = np.squeeze(f_first['embeddings'][:]).shape[1]\n",
    "        embed_dtype = f_first['embeddings'].dtype\n",
    "        url_dtype = f_first['urls'].dtype\n",
    "\n",
    "    # Create the output file and initialize extendable datasets.\n",
    "    with h5py.File(output_file, 'w') as f_output:\n",
    "        # Initialize 'urls' dataset with zero length, maxshape=(None,) allows extension.\n",
    "        f_output.create_dataset(\n",
    "            'urls',\n",
    "            shape=(0,),\n",
    "            maxshape=(None,),\n",
    "            dtype=url_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "        # Initialize 'embeddings' dataset with zero length, maxshape=(None, embed_shape) allows extension.\n",
    "        f_output.create_dataset(\n",
    "            'embeddings',\n",
    "            shape=(0, embed_shape),\n",
    "            maxshape=(None, embed_shape),\n",
    "            dtype=embed_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "\n",
    "    pbar = tqdm(total = len(input_list), desc=\"Merging\")\n",
    "\n",
    "    # 2. Loop through input files and append data\n",
    "    # Open the output file in append mode ('a') for modification.\n",
    "    with h5py.File(output_file, 'a') as f_output:\n",
    "        for file_path in input_list:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"⚠️ File not found: {file_path}. Skipping.\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with h5py.File(file_path, 'r') as f_input:\n",
    "                    # Read data from the current input file.\n",
    "                    current_urls = f_input['urls'][:]\n",
    "                    current_embeddings = f_input['embeddings'][:]\n",
    "                    current_embeddings = np.squeeze(current_embeddings)\n",
    "                    num_records = current_urls.shape[0]\n",
    "\n",
    "                    if num_records == 0:\n",
    "                        continue\n",
    "\n",
    "                    dset_urls = f_output['urls']\n",
    "                    dset_embeddings = f_output['embeddings']\n",
    "\n",
    "                    new_size = total_records + num_records\n",
    "\n",
    "                    # Resize the datasets in the output file to accommodate new records.\n",
    "                    dset_urls.resize(new_size, axis=0)\n",
    "                    dset_embeddings.resize(new_size, axis=0)\n",
    "\n",
    "                    # Write the current file's data into the newly reserved space.\n",
    "                    dset_urls[total_records:new_size] = current_urls\n",
    "                    dset_embeddings[total_records:new_size] = current_embeddings\n",
    "\n",
    "                    # Update the running total of merged records.\n",
    "                    total_records = new_size\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle potential errors during file reading or resizing.\n",
    "                print(f\"❌ Error processing file {file_path}: {e}. Skipping this file.\")\n",
    "\n",
    "            pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5dfc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = max_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c4a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = 10000\n",
    "chunk = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1dc21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE = \"../.cache\"\n",
    "MODEL_LIST = [  \"openai/clip-vit-base-patch32\",\n",
    "                \"openai/clip-vit-base-patch16\",\n",
    "                \"openai/clip-vit-large-patch14\",\n",
    "                \"openai/clip-vit-large-patch14-336\" ]\n",
    "MODEL = MODEL_LIST[3]\n",
    "NUM_WORKERS = min(16, int(chunk/200))\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028279a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageDataset(Dataset):\n",
    "    def __init__(self, image_ids, processor):\n",
    "        self.image_ids = image_ids      \n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]  \n",
    "        image_path = f\"{CACHE}/images/{image_id}.jpg\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return self.processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e668c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘OUTPUT’: File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mkdir\", \"OUTPUT\"])\n",
    "processor = CLIPProcessor.from_pretrained(MODEL, use_fast=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(MODEL).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a8673f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images: 100%|█████████████████| 10000/10000 [02:54<00:00, 57.29it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(CACHE + \"/list_images.txt\", \"w\") as f:\n",
    "    for i in range(start, end + 1):\n",
    "        f.write(get_row(i)[\"subset\"] + \"/\" + get_row(i)[\"imageid\"] + \"\\n\")\n",
    "!python ../.cache/downloader.py ../.cache/list_images.txt --download_folder=../.cache/images --num_processes=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11510e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "image_ids = [get_row(j)[\"imageid\"] for j in range(start, start + chunk)]\n",
    "print(len(image_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a6f98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268876cce53a41e4b33b2c28693acebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 1 to 1000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe64780b7364c2ea3de8c4af381a4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 1001 to 2000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd3afe8254f4152927796756c67b972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 2001 to 3000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af717799d0b242579d34024d31eea56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 3001 to 4000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52b9e9c343d4d6f8d57104e6e2304d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 4001 to 5000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1fcae2e8064512b8f6a02d5f2c4a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 5001 to 6000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97520bd861b24581a9ea29b7948b7ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 6001 to 7000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02dcb14c00b4a988ef03738aa280b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 7001 to 8000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89206b9837e7431eb50d0c8c36bf9a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 8001 to 9000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d92b5c4885349a49008a8f7838de9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 9001 to 10000:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(start, end + 1, chunk):\n",
    "    batch_rows = get_batch_rows(i, i + chunk)\n",
    "    \n",
    "    if not batch_rows:\n",
    "        continue\n",
    "\n",
    "    current_image_ids = [row[\"imageid\"] for row in batch_rows]\n",
    "    \n",
    "    Data = CLIPImageDataset(image_ids=current_image_ids, processor=processor)\n",
    "    LoadData = DataLoader(Data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    all_image_embeddings = []\n",
    "\n",
    "    for batch in tqdm(LoadData, desc=f\"Processing images {i} to {i + chunk - 1}\"):\n",
    "        inputs = {k: v.squeeze(1).to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        \n",
    "        image_embeddings = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        all_image_embeddings.append(image_embeddings.cpu())\n",
    "\n",
    "        del inputs\n",
    "        del image_features\n",
    "        del image_embeddings\n",
    "\n",
    "    all_image_embeddings_tensor = torch.cat(all_image_embeddings, dim=0)\n",
    "    \n",
    "    del all_image_embeddings \n",
    "\n",
    "    output_path = f\"OUTPUT/{MODEL[7:]}_Images_Embedded_{i}_to_{i + chunk - 1}.h5\"\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        all_embeddings_numpy = all_image_embeddings_tensor.numpy()\n",
    "        current_urls = [row[\"originalurl\"] for row in batch_rows]\n",
    "        with h5py.File(output_path, \"w\") as outfile:\n",
    "            dt = h5py.string_dtype(encoding='utf-8')\n",
    "            outfile.create_dataset(\"urls\", data=current_urls, dtype=dt)\n",
    "            outfile.create_dataset(\"embeddings\", data=all_embeddings_numpy)\n",
    "        \n",
    "        del all_embeddings_numpy\n",
    "\n",
    "    del all_image_embeddings_tensor\n",
    "    del Data\n",
    "    del LoadData\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1034e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5975443d4f344b179dad7c6173e27c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. MERGE CHUNKED HDF5 FILES\n",
    "# Generate a list of all HDF5 chunk files created during the embedding process.\n",
    "file_chunks = [\n",
    "    f\"OUTPUT/{MODEL[7:]}_Images_Embedded_{i}_to_{i + chunk - 1}.h5\"\n",
    "    for i in range(start, end + 1, chunk)\n",
    "]\n",
    "\n",
    "# Define the final, consolidated HDF5 file path.\n",
    "file_gop_cuoi = f\"OUTPUT/{MODEL[7:]}_Images_Embedded_{start}_to_{end}.h5\"\n",
    "\n",
    "# Call the function to merge all chunk files into the single final file.\n",
    "merge_HDF5_files(file_chunks, file_gop_cuoi)\n",
    "\n",
    "# 2. CLEANUP: REMOVE TEMPORARY CHUNKS\n",
    "# Iterate through the indices used to generate the chunk files.\n",
    "for chunk_path in file_chunks:\n",
    "    try:\n",
    "        # Delete the individual temporary HDF5 chunk file to free up disk space.\n",
    "        os.remove(chunk_path)\n",
    "    except FileNotFoundError:\n",
    "        # Handle case where the file might have been skipped or already deleted.\n",
    "        print(f\"⚠️ Warning: Chunk file not found during cleanup: {chunk_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c8bc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 512)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "r = h5py.File(file_gop_cuoi, \"r\")\n",
    "print(r[\"embeddings\"].shape)\n",
    "print(r[\"urls\"].shape)\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19f649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WEB-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
