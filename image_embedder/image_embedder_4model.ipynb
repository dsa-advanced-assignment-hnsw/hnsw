{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34ea2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import psycopg\n",
    "import torch\n",
    "import h5py\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from psycopg.rows import dict_row\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46f935e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"0909231769\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "071e29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index() -> int:\n",
    "    with conn.cursor(row_factory=dict_row) as cur:\n",
    "        cur.execute(\"SELECT COUNT(*) AS total_rows FROM raw_image\")\n",
    "        result = cur.fetchone()\n",
    "    return result['total_rows']\n",
    "\n",
    "def get_row(index: int) -> dict | None:\n",
    "    with conn.cursor(row_factory=dict_row) as cur: \n",
    "        cur.execute(\"SELECT * FROM raw_image WHERE idx = %s\", (index,))\n",
    "        return cur.fetchone()\n",
    "    \n",
    "def get_batch_rows(start_idx: int, end_idx: int) -> list[dict]:\n",
    "    with conn.cursor(row_factory=dict_row) as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT idx, imageid, subset, originalurl \n",
    "            FROM raw_image \n",
    "            WHERE idx >= %s AND idx < %s \n",
    "            ORDER BY idx\n",
    "        \"\"\", (start_idx, end_idx))\n",
    "        return cur.fetchall()\n",
    "    \n",
    "def merge_HDF5_files(input_list, output_file):\n",
    "    if not input_list:\n",
    "        print(\"❌ Error: Input file list is empty.\")\n",
    "        return\n",
    "    total_records = 0\n",
    "    first_file = None\n",
    "    for f_path in input_list:\n",
    "        if os.path.exists(f_path):\n",
    "            first_file = f_path\n",
    "            break\n",
    "    if not first_file:\n",
    "        print(\"❌ Error: No valid input files found.\")\n",
    "        return\n",
    "    with h5py.File(first_file, 'r') as f_first:\n",
    "        embed_shape = np.squeeze(f_first['embeddings'][:]).shape[1]\n",
    "        embed_dtype = f_first['embeddings'].dtype\n",
    "        url_dtype = f_first['urls'].dtype\n",
    "    with h5py.File(output_file, 'w') as f_output:\n",
    "        f_output.create_dataset(\n",
    "            'urls',\n",
    "            shape=(0,),\n",
    "            maxshape=(None,),\n",
    "            dtype=url_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "        f_output.create_dataset(\n",
    "            'embeddings',\n",
    "            shape=(0, embed_shape),\n",
    "            maxshape=(None, embed_shape),\n",
    "            dtype=embed_dtype,\n",
    "            chunks=True\n",
    "        )\n",
    "    pbar = tqdm(total = len(input_list), desc=\"Merging\")\n",
    "    with h5py.File(output_file, 'a') as f_output:\n",
    "        for file_path in input_list:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"⚠️ File not found: {file_path}. Skipping.\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            try:\n",
    "                with h5py.File(file_path, 'r') as f_input:\n",
    "                    current_urls = f_input['urls'][:]\n",
    "                    current_embeddings = f_input['embeddings'][:]\n",
    "                    current_embeddings = np.squeeze(current_embeddings)\n",
    "                    num_records = current_urls.shape[0]\n",
    "                    if num_records == 0:\n",
    "                        continue\n",
    "                    dset_urls = f_output['urls']\n",
    "                    dset_embeddings = f_output['embeddings']\n",
    "                    new_size = total_records + num_records\n",
    "                    dset_urls.resize(new_size, axis=0)\n",
    "                    dset_embeddings.resize(new_size, axis=0)\n",
    "                    dset_urls[total_records:new_size] = current_urls\n",
    "                    dset_embeddings[total_records:new_size] = current_embeddings\n",
    "                    total_records = new_size\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing file {file_path}: {e}. Skipping this file.\")\n",
    "            pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 600001\n",
    "end = 700000\n",
    "chunk = 10000\n",
    "end = min(end, max_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1dc21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST = [  \"openai/clip-vit-base-patch32\", \n",
    "                \"openai/clip-vit-base-patch16\",\n",
    "                \"openai/clip-vit-large-patch14\",\n",
    "                \"openai/clip-vit-large-patch14-336\" ]\n",
    "MODEL = MODEL_LIST[0]\n",
    "CACHE = \"../.cache\"\n",
    "OUTPUT = f\"../.cache/{MODEL[7:]}/image_embeddings\"\n",
    "NUM_WORKERS = min(16, int(chunk/200))\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "028279a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPImageDataset(Dataset):\n",
    "    def __init__(self, image_ids, processor):\n",
    "        self.image_ids = image_ids      \n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]  \n",
    "        image_path = f\"{CACHE}/images/{image_id}.jpg\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return self.processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e668c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../.cache/clip-vit-base-patch32’: File exists\n",
      "mkdir: cannot create directory ‘../.cache/clip-vit-base-patch32/image_embeddings’: File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mkdir\", f\"../.cache/{MODEL[7:]}\"])\n",
    "subprocess.run([\"mkdir\", OUTPUT])\n",
    "processor = CLIPProcessor.from_pretrained(MODEL, use_fast=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(MODEL).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a8673f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images: 100%|███████████████| 100000/100000 [22:39<00:00, 73.56it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(CACHE + \"/list_images.txt\", \"w\") as f:\n",
    "    for i in range(start, end + 1):\n",
    "        f.write(get_row(i)[\"subset\"] + \"/\" + get_row(i)[\"imageid\"] + \"\\n\")\n",
    "!python ../.cache/downloader.py ../.cache/list_images.txt --download_folder=../.cache/images --num_processes=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55a6f98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9716d16e5cb748f9bbb6f90cad0232d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 500001 to 510000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9940ba6adb7c4414bc7080eabb45c41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 510001 to 520000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b3bc660a9a44c3b586fcf0a3bf8a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 520001 to 530000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468117eb969f46178d713484962e7166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 530001 to 540000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6a424aa44e4a5ea408fb96a428ae08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 540001 to 550000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce3124bbab545c2b6b945f5efd93ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 550001 to 560000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e02a8b852340fa97782b8234c4f3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 560001 to 570000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75742c1a12bc47a5b7ca0454a6e059f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 570001 to 580000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56ab2ac0a4f4881b0f849fe29b4a588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 580001 to 590000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e55d43711844d7aaeeefcb7a4db4176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images 590001 to 600000:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(start, end + 1, chunk):\n",
    "    batch_rows = get_batch_rows(i, i + chunk)\n",
    "    if not batch_rows:\n",
    "        continue\n",
    "    current_image_ids = [row[\"imageid\"] for row in batch_rows]\n",
    "    Data = CLIPImageDataset(image_ids=current_image_ids, processor=processor)\n",
    "    LoadData = DataLoader(Data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    all_image_embeddings = []\n",
    "    for batch in tqdm(LoadData, desc=f\"Processing images {i} to {i + chunk - 1}\"):\n",
    "        inputs = {k: v.squeeze(1).to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        image_embeddings = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        all_image_embeddings.append(image_embeddings.cpu())\n",
    "        del inputs\n",
    "        del image_features\n",
    "        del image_embeddings\n",
    "    all_image_embeddings_tensor = torch.cat(all_image_embeddings, dim=0)\n",
    "    del all_image_embeddings \n",
    "    output_path = OUTPUT + f\"/{MODEL[7:]}_Images_Embedded_{i}_to_{i + chunk - 1}.h5\"\n",
    "    if not os.path.exists(output_path):\n",
    "        all_embeddings_numpy = all_image_embeddings_tensor.numpy()\n",
    "        current_urls = [row[\"originalurl\"] for row in batch_rows]\n",
    "        with h5py.File(output_path, \"w\") as outfile:\n",
    "            dt = h5py.string_dtype(encoding='utf-8')\n",
    "            outfile.create_dataset(\"urls\", data=current_urls, dtype=dt)\n",
    "            outfile.create_dataset(\"embeddings\", data=all_embeddings_numpy)\n",
    "        del all_embeddings_numpy\n",
    "    del all_image_embeddings_tensor\n",
    "    del Data\n",
    "    del LoadData\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba1034e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8bf3bf60f14a3393df80715c4f9474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_chunks = [\n",
    "    OUTPUT + f\"/{MODEL[7:]}_Images_Embedded_{i}_to_{i + chunk - 1}.h5\"\n",
    "    for i in range(start, end + 1, chunk)\n",
    "]\n",
    "file_gop_cuoi = OUTPUT + f\"/{MODEL[7:]}_Images_Embedded_{start}_to_{end}.h5\"\n",
    "merge_HDF5_files(file_chunks, file_gop_cuoi)\n",
    "for chunk_path in file_chunks:\n",
    "    try:\n",
    "        os.remove(chunk_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Warning: Chunk file not found during cleanup: {chunk_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19f649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WEB-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
