{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5752a47d",
   "metadata": {},
   "source": [
    "# 1. Configuration and Data Loading\n",
    "\n",
    "This cell sets up the necessary libraries, defines the input file configuration (grouped by dimension), and includes the function to load and sample data using h5py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: File model_a.h5 not found. Please check the file name.\n",
      "ERROR: File model_b.h5 not found. Please check the file name.\n",
      "ERROR: File model_c.h5 not found. Please check the file name.\n",
      "ERROR: File model_d.h5 not found. Please check the file name.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# --- Cáº¤U HÃŒNH ---\n",
    "# Note: Báº¡n cáº§n Ä‘áº£m báº£o cÃ¡c tá»‡p nÃ y tá»“n táº¡i trong thÆ° má»¥c lÃ m viá»‡c.\n",
    "file_configs = {\n",
    "    # NhÃ³m 512 chiá»u\n",
    "    'Model_A_512': {'filename': '.cache/clip-vit-base-patch32/image_embeddings/clip-vit-base-patch32_Images_Embedded_1_to_100000.h5', 'dim': 512, 'group': '512D'},\n",
    "    'Model_B_512': {'filename': '.cache/clip-vit-base-patch16/image_embeddings/clip-vit-base-patch16_Images_Embedded_1_to_100000.h5', 'dim': 512, 'group': '512D'},\n",
    "    # NhÃ³m 768 chiá»u\n",
    "    'Model_C_768': {'filename': '.cache/clip-vit-large-patch14/image_embeddings/clip-vit-large-patch14_Images_Embedded_1_to_100000.h5', 'dim': 768, 'group': '768D'},\n",
    "    'Model_D_768': {'filename': '.cache/clip-vit-large-patch14-336/image_embeddings/clip-vit-large-patch14-336_Images_Embedded_1_to_10000.h5', 'dim': 768, 'group': '768D'},\n",
    "}\n",
    "\n",
    "EMBEDDING_KEY = 'embeddings' # Field name containing the embedding vectors in the H5 file\n",
    "SAMPLE_SIZE = 10000 \n",
    "NUM_CLUSTERS_KMEANS = 4 # Assumed number of clusters for discriminability evaluation\n",
    "\n",
    "def load_and_sample_embeddings(filename: str, model_name: str, expected_dim: int, sample_size: int) -> np.ndarray | None:\n",
    "    \"\"\"Loads and samples embeddings from H5 files using h5py.\"\"\"\n",
    "    try:\n",
    "        # --- REAL LOADING LOGIC FROM H5PY ---\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            if EMBEDDING_KEY not in f:\n",
    "                 raise KeyError(f\"Field '{EMBEDDING_KEY}' does not exist in the file.\")\n",
    "            embeddings = f[EMBEDDING_KEY][:]\n",
    "        # ------------------------------------\n",
    "\n",
    "        if embeddings.shape[1] != expected_dim:\n",
    "            print(f\"Size error: {model_name} has dimension {embeddings.shape[1]}, which does not match {expected_dim}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        if embeddings.shape[0] > sample_size:\n",
    "            print(f\"[{model_name}]: Randomly sampling {sample_size} points.\")\n",
    "            indices = np.random.choice(embeddings.shape[0], sample_size, replace=False)\n",
    "            embeddings = embeddings[indices]\n",
    "        \n",
    "        # Normalization (Crucial if your vectors are not already unit length, e.g., for Cosine Similarity)\n",
    "        norm_check = np.linalg.norm(embeddings, axis=1)\n",
    "        if not np.allclose(norm_check, 1.0, atol=1e-3):\n",
    "             embeddings = embeddings / norm_check[:, np.newaxis]\n",
    "\n",
    "        return embeddings\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File {filename} not found. Please check the file name.\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: {e} in file {filename}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "         print(f\"OTHER ERROR loading {filename}: {e}\")\n",
    "         return None\n",
    "\n",
    "# --- LOAD AND GROUP DATA ---\n",
    "data_by_group: Dict[str, List[pd.DataFrame]] = {'512D': [], '768D': []}\n",
    "model_embeddings: Dict[str, np.ndarray] = {}\n",
    "stats: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "for model_name, config in file_configs.items():\n",
    "    embeddings = load_and_sample_embeddings(\n",
    "        config['filename'],\n",
    "        model_name,\n",
    "        config['dim'],\n",
    "        SAMPLE_SIZE\n",
    "    )\n",
    "\n",
    "    if embeddings is not None:\n",
    "        model_embeddings[model_name] = embeddings\n",
    "        \n",
    "        data_by_group[config['group']].append(\n",
    "            pd.DataFrame({\n",
    "                'embedding': list(embeddings),\n",
    "                'model': model_name\n",
    "            })\n",
    "        )\n",
    "\n",
    "        stats[model_name] = {\n",
    "            'Count': embeddings.shape[0],\n",
    "            'Embedding_Dim': embeddings.shape[1],\n",
    "            'Mean_Norm': np.mean(np.linalg.norm(embeddings, axis=1)), # Average vector magnitude\n",
    "            'Mean': np.mean(embeddings),\n",
    "            'StdDev': np.std(embeddings),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6766ac",
   "metadata": {},
   "source": [
    "# 2. Statistical and Geometric (PCA) Comparison\n",
    "\n",
    "This cell displays the basic statistical table and plots the PCA for each dimension group (512D and 768D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e73fca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------\n",
      "BASIC STATISTICAL COMPARISON TABLE OF EMBEDDINGS\n",
      "-------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "-------------------------------------------\n",
      "\n",
      "âš¡ï¸ Geometric Evaluation using PCA (by dimension group)...\n"
     ]
    }
   ],
   "source": [
    "# --- A. STATISTICAL COMPARISON ---\n",
    "print(\"\\n-------------------------------------------\")\n",
    "print(\"BASIC STATISTICAL COMPARISON TABLE OF EMBEDDINGS\")\n",
    "print(\"-------------------------------------------\")\n",
    "stats_df = pd.DataFrame(stats).T\n",
    "print(stats_df)\n",
    "print(\"-------------------------------------------\\n\")\n",
    "\n",
    "# --- B. GEOMETRIC COMPARISON (PCA) ---\n",
    "print(\"âš¡ï¸ Geometric Evaluation using PCA (by dimension group)...\")\n",
    "for group_name, data_list in data_by_group.items():\n",
    "    if not data_list:\n",
    "        continue\n",
    "\n",
    "    combined_df = pd.concat(data_list, ignore_index=True)\n",
    "    X = np.array(combined_df['embedding'].tolist())\n",
    "\n",
    "    # Data normalization \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "    pca_df['model'] = combined_df['model']\n",
    "    \n",
    "    # Visualization (using plt.show() to display directly)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, group in pca_df.groupby('model'):\n",
    "        plt.scatter(group['PC1'], group['PC2'], label=name, alpha=0.6, s=15)\n",
    "\n",
    "    total_variance = pca.explained_variance_ratio_.sum() * 100\n",
    "    plt.title(f'Embedding Space Comparison using PCA ({group_name}) - Total Variance: {total_variance:.2f}%')\n",
    "    plt.xlabel(f'Principal Component 1 (Explained: {pca.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "    plt.ylabel(f'Principal Component 2 (Explained: {pca.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show() # DISPLAY PLOT DIRECTLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9971531",
   "metadata": {},
   "source": [
    "# 3. Embedding Quality Evaluation (Clustering Proxy)\n",
    "\n",
    "This cell uses Clustering metrics (Silhouette Score and Inertia) as a proxy to assess the internal structure and quality of the embedding spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee92e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Embedding Quality Evaluation (Clustering Proxy)...\n",
      "\n",
      "-------------------------------------------\n",
      "CLUSTERING EVALUATION RESULTS (ASSUMING K=4)\n",
      "-------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "--- Meaning of Metrics ---\n",
      "* **Inertia (Cluster Tightness):** Sum of squared distances to the nearest cluster center. Lower is better.\n",
      "* **Silhouette Score (Discriminability):** Measures similarity to its own cluster compared to other clusters. Closer to +1 is better.\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- C. PERFORMANCE EVALUATION (CLUSTERING PROXY) ---\n",
    "\n",
    "def evaluate_embedding_quality(embeddings: np.ndarray, num_clusters: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses K-Means Clustering and Silhouette Score as a proxy to evaluate embedding quality.\n",
    "    \"\"\"\n",
    "    if len(embeddings) < num_clusters or num_clusters < 2:\n",
    "        return {'Inertia': np.nan, 'Silhouette_Score': np.nan}\n",
    "    \n",
    "    try:\n",
    "        # Random sampling for K-Means to speed up computation\n",
    "        n_samples_for_kmeans = min(2000, embeddings.shape[0])\n",
    "        idx = np.random.choice(embeddings.shape[0], n_samples_for_kmeans, replace=False)\n",
    "        embeddings_sample = embeddings[idx]\n",
    "\n",
    "        # 1. Perform K-Means\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings_sample)\n",
    "        \n",
    "        # 2. Calculate Inertia (Cluster Tightness - lower is better)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        # 3. Calculate Silhouette Score (Discriminability - closer to 1 is better)\n",
    "        if len(np.unique(labels)) > 1:\n",
    "            silhouette = silhouette_score(embeddings_sample, labels)\n",
    "        else:\n",
    "            silhouette = 0.0\n",
    "            \n",
    "        return {'Inertia': inertia, 'Silhouette_Score': silhouette}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'Inertia': np.nan, 'Silhouette_Score': np.nan}\n",
    "\n",
    "\n",
    "print(\"\\nðŸŽ¯ Embedding Quality Evaluation (Clustering Proxy)...\")\n",
    "\n",
    "performance_results = {}\n",
    "for model_name, embeddings in model_embeddings.items():\n",
    "    results = evaluate_embedding_quality(embeddings, NUM_CLUSTERS_KMEANS)\n",
    "    performance_results[model_name] = results\n",
    "\n",
    "# Print performance evaluation results\n",
    "print(\"\\n-------------------------------------------\")\n",
    "print(f\"CLUSTERING EVALUATION RESULTS (ASSUMING K={NUM_CLUSTERS_KMEANS})\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "perf_df = pd.DataFrame(performance_results).T\n",
    "print(perf_df)\n",
    "\n",
    "print(\"\\n--- Meaning of Metrics ---\")\n",
    "print(\"* **Inertia (Cluster Tightness):** Sum of squared distances to the nearest cluster center. Lower is better.\")\n",
    "print(\"* **Silhouette Score (Discriminability):** Measures similarity to its own cluster compared to other clusters. Closer to +1 is better.\")\n",
    "print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1c0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WEB-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
